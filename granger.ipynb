{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: CTSH, Stationary: True, p-value: 0.0374\n",
      "Column: SIRI, Stationary: True, p-value: 0.0203\n",
      "Column: Rolling_Correlation_Coefficient, Stationary: True, p-value: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import numpy as np\n",
    "\n",
    "# Caricamento del DataFrame dal file pickle\n",
    "file_path = 'helpermodules/data/final_dataframe.pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "\n",
    "# Funzione per verificare la stazionarietà\n",
    "def check_stationarity(series, alpha=0.05):\n",
    "    series = series.replace([float('inf'), float('-inf')], float('nan')).dropna()\n",
    "    if len(series) == 0:\n",
    "        raise ValueError(\"Series is empty after cleaning.\")\n",
    "    result = adfuller(series)\n",
    "    p_value = result[1]\n",
    "    return p_value < alpha, p_value\n",
    "\n",
    "# Applicazione del test\n",
    "stationarity_results = {}\n",
    "for column in df.columns:\n",
    "    try:\n",
    "        is_stationary, p_value = check_stationarity(df[column])\n",
    "        stationarity_results[column] = {\"is_stationary\": is_stationary, \"p_value\": p_value}\n",
    "    except ValueError as e:\n",
    "        stationarity_results[column] = {\"is_stationary\": False, \"p_value\": None, \"error\": str(e)}\n",
    "\n",
    "# Stampa dei risultati\n",
    "for column, result in stationarity_results.items():\n",
    "    if \"error\" in result:\n",
    "        print(f\"Column: {column}, Error: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"Column: {column}, Stationary: {result['is_stationary']}, p-value: {result['p_value']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair: CTSH -> CTSH\n",
      "P-Values: [nan, nan, nan, nan, nan]\n",
      "F-Statistics: [nan, nan, nan, nan, nan]\n",
      "\n",
      "Pair: CTSH -> SIRI\n",
      "P-Values: [0.7528294938733242, 0.7622315991700742, 0.6196361981578298, 0.7789057872092702, 0.8782735176265299]\n",
      "F-Statistics: [0.09917186501162666, 0.2715090898997547, 0.5928291530709265, 0.441210592649645, 0.356600084170411]\n",
      "\n",
      "Pair: SIRI -> CTSH\n",
      "P-Values: [0.32011330990525866, 0.0079499705520038, 0.004805081789719483, 0.009310850874367923, 0.01767263028030367]\n",
      "F-Statistics: [0.9885402108828549, 4.835936877349672, 4.309304005275755, 3.3613340148433615, 2.7397003836349443]\n",
      "\n",
      "Pair: SIRI -> SIRI\n",
      "P-Values: [nan, nan, nan, nan, nan]\n",
      "F-Statistics: [nan, nan, nan, nan, nan]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edo/nasdaq_causal-analysis_lstm/helpermodules/granger_casuality.py:52: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data = self.dataframe[[ticker_x, ticker_y]].fillna(method='ffill').fillna(method='bfill')\n",
      "/Users/edo/Library/Python/3.9/lib/python/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
      "  warnings.warn(\n",
      "/Users/edo/nasdaq_causal-analysis_lstm/helpermodules/granger_casuality.py:52: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data = self.dataframe[[ticker_x, ticker_y]].fillna(method='ffill').fillna(method='bfill')\n",
      "/Users/edo/Library/Python/3.9/lib/python/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from helpermodules.granger_casuality import GrangerCausalityAnalysis\n",
    "\n",
    "# Carica il DataFrame\n",
    "file_path = 'helpermodules/data/final_dataframe.pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "\n",
    "# Pulizia del DataFrame\n",
    "df_cleaned = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "valid_columns = [col for col in df_cleaned.columns if 'Rolling_Correlation_Coefficient' not in col]\n",
    "df_filtered = df_cleaned[valid_columns]\n",
    "\n",
    "\n",
    "# Inizializza l'analisi di Granger\n",
    "granger_analysis = GrangerCausalityAnalysis(dataframe=df_filtered, max_lag=5)\n",
    "\n",
    "# Calcola la causalità di Granger\n",
    "results = granger_analysis.calculate_granger_causality()\n",
    "\n",
    "# Stampa i risultati\n",
    "for (ticker_x, ticker_y), result in results.items():\n",
    "    print(f\"Pair: {ticker_x} -> {ticker_y}\")\n",
    "    print(f\"P-Values: {result['p_values']}\")\n",
    "    print(f\"F-Statistics: {result['f_statistics']}\\n\")\n",
    "\n",
    "# Identifica le coppie significative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne di df_two_stocks: Index(['CTSH', 'SIRI'], dtype='object')\n",
      "Forma di df_two_stocks: (17326, 2)\n",
      "Prime righe di df_two_stocks:\n",
      "                       CTSH    SIRI\n",
      "2024-02-26 09:45:00  78.93  4.5735\n",
      "2024-02-26 09:46:00  78.93  4.5735\n",
      "2024-02-26 09:47:00  78.93  4.5735\n",
      "2024-02-26 09:48:00  78.93  4.5735\n",
      "2024-02-26 09:49:00  78.93  4.5735\n",
      "Controllo NaN in df_two_stocks:\n",
      " CTSH    0\n",
      "SIRI    0\n",
      "dtype: int64\n",
      "Errore durante il calcolo della causalità:\n",
      "x should have 2 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/8n/jpnc_m256z7gksjpzfgjfn440000gn/T/ipykernel_6057/2749659524.py\", line 42, in <module>\n",
      "    results = nonlinear_granger.calculate_nonlinear_nn_causality(\n",
      "  File \"/Users/edo/nasdaq_causal-analysis_lstm/helpermodules/nonlin_granger_casuality.py\", line 80, in calculate_nonlinear_nn_causality\n",
      "    print(f\"Validation data shape for {ticker_x} -> {ticker_y}:\", data_val.shape)\n",
      "  File \"/Users/edo/Library/Python/3.9/lib/python/site-packages/nonlincausality/nonlincausality.py\", line 527, in nonlincausalityNN\n",
      "    results = run_nonlincausality(\n",
      "  File \"/Users/edo/Library/Python/3.9/lib/python/site-packages/nonlincausality/nonlincausality.py\", line 141, in run_nonlincausality\n",
      "    check_input(\n",
      "  File \"/Users/edo/Library/Python/3.9/lib/python/site-packages/nonlincausality/utils.py\", line 43, in check_input\n",
      "    raise Exception(\"x should have 2 columns.\")\n",
      "Exception: x should have 2 columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from helpermodules.nonlin_granger_casuality import NonlinearNNGrangerCausalityAnalysis\n",
    "\n",
    "# Carica il dataset\n",
    "file_path = 'helpermodules/data/final_dataframe.pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "\n",
    "# Pulizia dei dati\n",
    "df_cleaned = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# Escludi colonne non necessarie\n",
    "valid_columns = [col for col in df_cleaned.columns if 'Rolling_Correlation_Coefficient' not in col]\n",
    "df_filtered = df_cleaned[valid_columns]\n",
    "\n",
    "# Specifica i titoli da analizzare\n",
    "stock_x = df_filtered.columns[0]  # Ad esempio, la prima colonna\n",
    "stock_y = df_filtered.columns[1]  # Ad esempio, la seconda colonna\n",
    "\n",
    "# Filtra solo i dati dei due titoli\n",
    "df_two_stocks = df_filtered[[stock_x, stock_y]]\n",
    "\n",
    "# Debug preliminare\n",
    "print(\"Colonne di df_two_stocks:\", df_two_stocks.columns)\n",
    "print(\"Forma di df_two_stocks:\", df_two_stocks.shape)\n",
    "print(\"Prime righe di df_two_stocks:\\n\", df_two_stocks.head())\n",
    "print(\"Controllo NaN in df_two_stocks:\\n\", df_two_stocks.isnull().sum())\n",
    "\n",
    "# Creazione di un DataFrame formattato con i dati concatenati\n",
    "df_formatted = df_two_stocks.dropna()\n",
    "\n",
    "# Inizializza l'analisi\n",
    "nonlinear_granger = NonlinearNNGrangerCausalityAnalysis(\n",
    "    dataframe=df_formatted,\n",
    "    max_lag=5,\n",
    "    nn_config=['d', 'dr', 'd', 'dr'],\n",
    "    nn_neurons=[100, 0.05, 100, 0.05]\n",
    ")\n",
    "\n",
    "# Calcola la causalità\n",
    "try:\n",
    "    results = nonlinear_granger.calculate_nonlinear_nn_causality(\n",
    "        epochs=[50, 50],\n",
    "        learning_rate=[0.0001, 0.00001],\n",
    "        batch_size=32\n",
    "    )\n",
    "    # Stampa i risultati per ciascun verso di causalità\n",
    "    print(f\"Risultati per {stock_x} -> {stock_y}:\")\n",
    "    print(f\"Causality Score: {results[(stock_x, stock_y)]['causality_score']}\")\n",
    "    print(f\"P-Value: {results[(stock_x, stock_y)]['p_value']}\")\n",
    "\n",
    "    print(f\"\\nRisultati per {stock_y} -> {stock_x}:\")\n",
    "    print(f\"Causality Score: {results[(stock_y, stock_x)]['causality_score']}\")\n",
    "    print(f\"P-Value: {results[(stock_y, stock_x)]['p_value']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(\"Errore durante il calcolo della causalità:\")\n",
    "    print(e)\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varianza CTSH: 15.557420272030233\n",
      "Varianza SIRI: 0.1980963557171658\n",
      "Shape of df_two_stocks: (9353, 2)\n",
      "Shape of lagged X: (9348, 5)\n",
      "Shape of lagged Y: (9348, 5)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "x should have 2 columns.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 39\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Analisi Granger\u001b[39;00m\n\u001b[1;32m     32\u001b[0m nonlinear_granger \u001b[38;5;241m=\u001b[39m NonlinearNNGrangerCausalityAnalysis(\n\u001b[1;32m     33\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mdf_two_stocks,\n\u001b[1;32m     34\u001b[0m     max_lag\u001b[38;5;241m=\u001b[39mlag,\n\u001b[1;32m     35\u001b[0m     nn_config\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     36\u001b[0m     nn_neurons\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m0.05\u001b[39m]\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 39\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mnonlinear_granger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_nonlinear_nn_causality\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00001\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\n\u001b[1;32m     43\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nasdaq_causal-analysis_lstm/helpermodules/nonlin_granger_casuality.py:80\u001b[0m, in \u001b[0;36mNonlinearNNGrangerCausalityAnalysis.calculate_nonlinear_nn_causality\u001b[0;34m(self, epochs, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     78\u001b[0m data_train, data_val \u001b[38;5;241m=\u001b[39m data_x[:train_size], data_x[train_size:]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Perform the nonlinear Granger causality test using the nonlincausalityNN method\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnlc_nn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Training data for ticker_x\u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxlag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_lag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Maximum lag for Granger causality\u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNN_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Neural network configuration\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNN_neurons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_neurons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of neurons and dropout rates\u001b[39;49;00m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Testing data for ticker_y\u001b[39;49;00m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of runs for the analysis\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of epochs per training phase\u001b[39;49;00m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Learning rate for each phase\u001b[39;49;00m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Batch size during training\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Validation data\u001b[39;49;00m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True to print verbose output during training\u001b[39;49;00m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to False to disable plotting\u001b[39;49;00m\n\u001b[1;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Store the results (causality score and p-value) in the dictionary\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults[(ticker_x, ticker_y)] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcausality_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcausality_score\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Causality score from the model\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_value\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_value\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# p-value from the test indicating statistical significance\u001b[39;00m\n\u001b[1;32m     99\u001b[0m }\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nonlincausality/nonlincausality.py:527\u001b[0m, in \u001b[0;36mnonlincausalityNN\u001b[0;34m(x, maxlag, NN_config, NN_neurons, x_test, run, z, z_test, epochs_num, learning_rate, batch_size_num, x_val, z_val, regularization, reg_alpha, callbacks, verbose, plot)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnonlincausalityNN\u001b[39m(\n\u001b[1;32m    445\u001b[0m     x,\n\u001b[1;32m    446\u001b[0m     maxlag,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    462\u001b[0m     plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    463\u001b[0m ):\n\u001b[1;32m    464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    525\u001b[0m \n\u001b[1;32m    526\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 527\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_nonlincausality\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mNN_architecture\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxlag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mNN_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mNN_neurons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mz_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mz_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregularization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreg_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nonlincausality/nonlincausality.py:141\u001b[0m, in \u001b[0;36mrun_nonlincausality\u001b[0;34m(network_architecture, x, maxlag, Network_layers, Network_neurons, Dense_layers, Dense_neurons, x_test, run, z, z_test, x_val, z_val, add_Dropout, Dropout_rate, epochs_num, learning_rate, batch_size_num, regularization, reg_alpha, callbacks, verbose, plot, functin_type)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Checking the correctness of the input arguments\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m \u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxlag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNetwork_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNetwork_neurons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense_neurons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_Dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctin_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# If maxlag is int the test is made for every integer  from 1 to maxlag\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maxlag, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nonlincausality/utils.py:43\u001b[0m, in \u001b[0;36mcheck_input\u001b[0;34m(x, maxlag, Network_layers, Network_neurons, Dense_layers, Dense_neurons, x_test, run, z, z_test, add_Dropout, Dropout_rate, epochs_num, learning_rate, batch_size_num, verbose, plot, functin_type)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx has wrong shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx should have 2 columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(x):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is some NaN in x.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: x should have 2 columns."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.tsatools import lagmat2ds\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from helpermodules.nonlin_granger_casuality import NonlinearNNGrangerCausalityAnalysis\n",
    "\n",
    "# Carica il dataset\n",
    "file_path = 'helpermodules/data/final_dataframe.pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "\n",
    "# Pulizia dei dati\n",
    "df_cleaned = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "df_two_stocks = df_cleaned[['CTSH', 'SIRI']]\n",
    "\n",
    "# Normalizzazione manuale\n",
    "scaler = StandardScaler()\n",
    "df_two_stocks_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_two_stocks),\n",
    "    columns=df_two_stocks.columns,\n",
    "    index=df_two_stocks.index\n",
    ")\n",
    "\n",
    "# Debug preliminare\n",
    "print(\"Varianza CTSH:\", df_two_stocks_scaled['CTSH'].var())\n",
    "print(\"Varianza SIRI:\", df_two_stocks_scaled['SIRI'].var())\n",
    "print(\"Shape of df_two_stocks_scaled:\", df_two_stocks_scaled.shape)\n",
    "\n",
    "# Verifica lagging\n",
    "lag = 5\n",
    "data_X = lagmat2ds(df_two_stocks_scaled['CTSH'].values, lag - 1, trim=\"both\")[:-1, :]\n",
    "data_Y = lagmat2ds(df_two_stocks_scaled['SIRI'].values, lag - 1, trim=\"both\")[:-1, :]\n",
    "print(\"Shape of lagged X:\", data_X.shape)\n",
    "print(\"Shape of lagged Y:\", data_Y.shape)\n",
    "\n",
    "# Analisi Granger\n",
    "nonlinear_granger = NonlinearNNGrangerCausalityAnalysis(\n",
    "    dataframe=df_two_stocks_scaled,\n",
    "    max_lag=lag,\n",
    "    nn_config=['d', 'dr', 'd', 'dr'],\n",
    "    nn_neurons=[100, 0.05, 100, 0.05]\n",
    ")\n",
    "\n",
    "# Debug all'interno della libreria\n",
    "try:\n",
    "    results = nonlinear_granger.calculate_nonlinear_nn_causality(\n",
    "        epochs=[50, 50],\n",
    "        learning_rate=[0.0001, 0.00001],\n",
    "        batch_size=32\n",
    "    )\n",
    "    print(\"Results:\", results)\n",
    "except Exception as e:\n",
    "    print(\"Errore durante il calcolo della causalità:\")\n",
    "    print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
